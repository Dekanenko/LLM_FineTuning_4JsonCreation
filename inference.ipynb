{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install unsloth\n",
    "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(os.getenv('HUGGINGFACE_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.748 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.14 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (2-15): 14 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Llama1B_FineTuned\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import generate_prompt\n",
    "\n",
    "input = {\"input\": \"You are a name and surname extractor. Your task is to identify and extract the names and surnames of characters from the given text.\\n### Input:\\nIn the quaint town of Eldridge, Sarah Quinn and Lucas Bennett stood at the park, deep in conversation. The autumn leaves crunched underfoot as they walked along the winding path. \\n\\n\\\"I can't believe we‚Äôre finally doing this,\\\" Lucas said, a nervous excitement in his voice.\\n\\nSarah smiled, brushing a loose strand of hair behind her ear. \\\"I know! After months of planning, it feels surreal.\\\"\\n\\nThey stopped by the old oak tree, its branches stretching wide like welcoming arms. ‚ÄúHave you thought about what you‚Äôll say?‚Äù Sarah asked, her blue eyes sparkling with mischief.\\n\\n‚ÄúJust that I appreciate everything he‚Äôs done for us,‚Äù Lucas replied, kicking a pebble out of their way.\\n\\nAs they watched the sun dip below the horizon, Jack Caldwell approached, his camera slung across his shoulder. ‚ÄúThe moment‚Äôs finally here, huh?‚Äù he grinned, ready to capture the proposal.\\n\\nSarah squeezed Lucas‚Äôs hand, a flutter of anticipation in her stomach as they prepared for a new chapter in their lives. Tonight, under the stars, they would begin a story of love that had been years in the making.\\n\\n### Instructions:\\n1. Extract characters' names and surnames clearly.\\n2. Ensure that the JSON file contains a list of unique name-surname pairs (no duplicates).\\n\\n### Format Instructions:\\n1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\\n2. Strictly adhere to the following JSON format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"$defs\\\": {\\\"FullName\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"Name of the character\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"surname\\\": {\\\"description\\\": \\\"Surname of the character\\\", \\\"title\\\": \\\"Surname\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"surname\\\"], \\\"title\\\": \\\"FullName\\\", \\\"type\\\": \\\"object\\\"}}, \\\"properties\\\": {\\\"fullname_list\\\": {\\\"description\\\": \\\"List of full names for every character\\\", \\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/FullName\\\"}, \\\"title\\\": \\\"Fullname List\\\", \\\"type\\\": \\\"array\\\"}}, \\\"required\\\": [\\\"fullname_list\\\"]}\\n```\"}\n",
    "input = {\"input\": \"You are a professional chef creating detailed recipes. Create a complete recipe with precise measurements and clear instructions.\\n### Instructions:\\n1. Provide a clear, descriptive title for the recipe.\\n2. List all ingredients with exact quantities and units.\\n3. Write step-by-step cooking instructions in a logical order.\\n4. Include total cooking time in minutes.\\n5. Specify the cuisine type.\\n6. Include nutrition facts when possible (calories, protein, fat, carbs).\\n\\n### Format Instructions:\\n1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\\n2. Strictly adhere to the following JSON format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"$defs\\\": {\\\"Ingredient\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"quantity\\\": {\\\"title\\\": \\\"Quantity\\\", \\\"type\\\": \\\"number\\\"}, \\\"unit\\\": {\\\"title\\\": \\\"Unit\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"quantity\\\", \\\"unit\\\"], \\\"title\\\": \\\"Ingredient\\\", \\\"type\\\": \\\"object\\\"}, \\\"Instruction\\\": {\\\"properties\\\": {\\\"step_number\\\": {\\\"title\\\": \\\"Step Number\\\", \\\"type\\\": \\\"integer\\\"}, \\\"description\\\": {\\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"step_number\\\", \\\"description\\\"], \\\"title\\\": \\\"Instruction\\\", \\\"type\\\": \\\"object\\\"}, \\\"NutritionFacts\\\": {\\\"properties\\\": {\\\"calories\\\": {\\\"title\\\": \\\"Calories\\\", \\\"type\\\": \\\"number\\\"}, \\\"protein\\\": {\\\"title\\\": \\\"Protein\\\", \\\"type\\\": \\\"number\\\"}, \\\"fat\\\": {\\\"title\\\": \\\"Fat\\\", \\\"type\\\": \\\"number\\\"}, \\\"carbs\\\": {\\\"title\\\": \\\"Carbs\\\", \\\"type\\\": \\\"number\\\"}}, \\\"required\\\": [\\\"calories\\\", \\\"protein\\\", \\\"fat\\\", \\\"carbs\\\"], \\\"title\\\": \\\"NutritionFacts\\\", \\\"type\\\": \\\"object\\\"}}, \\\"properties\\\": {\\\"title\\\": {\\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"cuisine\\\": {\\\"title\\\": \\\"Cuisine\\\", \\\"type\\\": \\\"string\\\"}, \\\"ingredients\\\": {\\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Ingredient\\\"}, \\\"title\\\": \\\"Ingredients\\\", \\\"type\\\": \\\"array\\\"}, \\\"instructions\\\": {\\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Instruction\\\"}, \\\"title\\\": \\\"Instructions\\\", \\\"type\\\": \\\"array\\\"}, \\\"total_time_minutes\\\": {\\\"title\\\": \\\"Total Time Minutes\\\", \\\"type\\\": \\\"integer\\\"}, \\\"nutrition\\\": {\\\"anyOf\\\": [{\\\"$ref\\\": \\\"#/$defs/NutritionFacts\\\"}, {\\\"type\\\": \\\"null\\\"}], \\\"default\\\": null}}, \\\"required\\\": [\\\"title\\\", \\\"cuisine\\\", \\\"ingredients\\\", \\\"instructions\\\", \\\"total_time_minutes\\\"]}\\n```\"}\n",
    "input = {\"input\": \"You are a visionary game designer tasked with creating an innovative and engaging game concept.\\n### Instructions:\\n1. Create a unique game concept that combines compelling gameplay mechanics with an engaging narrative.\\n2. Design 2-5 memorable characters with distinct personalities, motivations, and story arcs.\\n3. Craft a rich setting that enhances the game's atmosphere and influences gameplay.\\n4. Develop an intriguing plot with meaningful conflicts and satisfying resolution.\\n5. Consider the game's genre and how it affects the overall experience.\\n6. Specify the target platform(s) and technical requirements.\\n7. Ensure the game idea is feasible and marketable while maintaining creativity.\\n\\n### Format Instructions:\\n1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\\n2. Strictly adhere to the following JSON format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"$defs\\\": {\\\"Character\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"Name of the character\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"age\\\": {\\\"description\\\": \\\"Age of the character\\\", \\\"title\\\": \\\"Age\\\", \\\"type\\\": \\\"integer\\\"}, \\\"gender\\\": {\\\"description\\\": \\\"Gender of the character\\\", \\\"title\\\": \\\"Gender\\\", \\\"type\\\": \\\"string\\\"}, \\\"role\\\": {\\\"description\\\": \\\"Role of the character\\\", \\\"title\\\": \\\"Role\\\", \\\"type\\\": \\\"string\\\"}, \\\"biography\\\": {\\\"description\\\": \\\"Biography of the character\\\", \\\"title\\\": \\\"Biography\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"age\\\", \\\"gender\\\", \\\"role\\\", \\\"biography\\\"], \\\"title\\\": \\\"Character\\\", \\\"type\\\": \\\"object\\\"}, \\\"Conflict\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"Name of the conflict\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"description\\\": {\\\"description\\\": \\\"Description of the conflict\\\", \\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"description\\\"], \\\"title\\\": \\\"Conflict\\\", \\\"type\\\": \\\"object\\\"}, \\\"Genre\\\": {\\\"enum\\\": [\\\"action\\\", \\\"comedy\\\", \\\"drama\\\", \\\"horror\\\", \\\"mystery\\\"], \\\"title\\\": \\\"Genre\\\", \\\"type\\\": \\\"string\\\"}, \\\"Plot\\\": {\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"Title of the plot\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"description\\\": {\\\"description\\\": \\\"Description of the plot\\\", \\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"description\\\"], \\\"title\\\": \\\"Plot\\\", \\\"type\\\": \\\"object\\\"}, \\\"Resolution\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"Name of the resolution\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"description\\\": {\\\"description\\\": \\\"Description of the resolution\\\", \\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"description\\\"], \\\"title\\\": \\\"Resolution\\\", \\\"type\\\": \\\"object\\\"}, \\\"Setting\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"Name of the setting\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"description\\\": {\\\"description\\\": \\\"Description of the setting\\\", \\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"description\\\"], \\\"title\\\": \\\"Setting\\\", \\\"type\\\": \\\"object\\\"}}, \\\"properties\\\": {\\\"genre\\\": {\\\"allOf\\\": [{\\\"$ref\\\": \\\"#/$defs/Genre\\\"}], \\\"description\\\": \\\"Genre of the game\\\"}, \\\"characters\\\": {\\\"description\\\": \\\"Characters of the game\\\", \\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Character\\\"}, \\\"title\\\": \\\"Characters\\\", \\\"type\\\": \\\"array\\\"}, \\\"setting\\\": {\\\"allOf\\\": [{\\\"$ref\\\": \\\"#/$defs/Setting\\\"}], \\\"description\\\": \\\"Setting of the game\\\"}, \\\"plot\\\": {\\\"allOf\\\": [{\\\"$ref\\\": \\\"#/$defs/Plot\\\"}], \\\"description\\\": \\\"Plot of the game\\\"}, \\\"conflict\\\": {\\\"allOf\\\": [{\\\"$ref\\\": \\\"#/$defs/Conflict\\\"}], \\\"description\\\": \\\"Conflict of the game\\\"}, \\\"resolution\\\": {\\\"allOf\\\": [{\\\"$ref\\\": \\\"#/$defs/Resolution\\\"}], \\\"description\\\": \\\"Resolution of the game\\\"}, \\\"description\\\": {\\\"description\\\": \\\"Description of the game\\\", \\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}, \\\"type\\\": {\\\"description\\\": \\\"Type of the game\\\", \\\"title\\\": \\\"Type\\\", \\\"type\\\": \\\"string\\\"}, \\\"platform\\\": {\\\"description\\\": \\\"Platform of the game\\\", \\\"title\\\": \\\"Platform\\\", \\\"type\\\": \\\"string\\\"}, \\\"release_date\\\": {\\\"description\\\": \\\"Release date of the game\\\", \\\"title\\\": \\\"Release Date\\\", \\\"type\\\": \\\"string\\\"}, \\\"rating\\\": {\\\"description\\\": \\\"Rating of the game\\\", \\\"title\\\": \\\"Rating\\\", \\\"type\\\": \\\"number\\\"}}, \\\"required\\\": [\\\"genre\\\", \\\"characters\\\", \\\"setting\\\", \\\"plot\\\", \\\"conflict\\\", \\\"resolution\\\", \\\"description\\\", \\\"type\\\", \\\"platform\\\", \\\"release_date\\\", \\\"rating\\\"]}\\n```\"}\n",
    "input = generate_prompt(input, train=False)\n",
    "# print(input)\n",
    "\n",
    "input_ids = tokenizer(input, padding=True, return_tensors = \"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 1024, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.langchain_llm_wrapper import LlamaLLM\n",
    "\n",
    "llm = LlamaLLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28041/1857286031.py:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = llm(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"title\": \"Creamy Garlic Parmesan Chicken\",\n",
      "  \"cuisine\": \"Italian\",\n",
      "  \"ingredients\": [\n",
      "    {\n",
      "      \"name\": \"Chicken breasts\",\n",
      "      \"quantity\": 4,\n",
      "      \"unit\": \"pieces\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Olive oil\",\n",
      "      \"quantity\": 2,\n",
      "      \"unit\": \"tablespoons\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Garlic, minced\",\n",
      "      \"quantity\": 4,\n",
      "      \"unit\": \"cloves\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Heavy cream\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit\": \"cup\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Parmesan cheese, grated\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit\": \"cup\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Salt\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit\": \"teaspoon\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Black pepper, freshly cracked\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit\": \"teaspoon\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Fresh parsley, chopped\",\n",
      "      \"quantity\": 2,\n",
      "      \"unit\": \"tablespoons\"\n",
      "    }\n",
      "  ],\n",
      "  \"instructions\": [\n",
      "    {\n",
      "      \"step_number\": 1,\n",
      "      \"description\": \"Heat the olive oil in a large skillet over medium heat.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 2,\n",
      "      \"description\": \"Season the chicken breasts with salt and black pepper on both sides.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 3,\n",
      "      \"description\": \"Add the chicken breasts to the skillet and cook for about 5-6 minutes on each side until golden brown. Remove the chicken from the skillet and set aside.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 4,\n",
      "      \"description\": \"In the same skillet, add the minced garlic and saut√© for about 1 minute until fragrant.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 5,\n",
      "      \"description\": \"Pour in the heavy cream and bring it to a simmer. Let it thicken slightly.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 6,\n",
      "      \"description\": \"Stir in the grated Parmesan cheese until melted and well combined.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 7,\n",
      "      \"description\": \"Add the cooked chicken back to the skillet and stir to coat with the creamy sauce.\"\n",
      "    },\n",
      "    {\n",
      "      \"step_number\": 8,\n",
      "      \"description\": \"Serve hot, garnished with chopped fresh parsley.\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_time_minutes\": 30,\n",
      "  \"nutrition\": {\n",
      "    \"calories\": 550,\n",
      "    \"protein\": 45,\n",
      "    \"fat\": 40,\n",
      "    \"carbs\": 6\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input = \"You are a professional chef creating detailed recipes. Create a complete recipe with precise measurements and clear instructions.\\n### Instructions:\\n1. Provide a clear, descriptive title for the recipe.\\n2. List all ingredients with exact quantities and units.\\n3. Write step-by-step cooking instructions in a logical order.\\n4. Include total cooking time in minutes.\\n5. Specify the cuisine type.\\n6. Include nutrition facts when possible (calories, protein, fat, carbs).\\n\\n### Format Instructions:\\n1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\\n2. Strictly adhere to the following JSON format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"$defs\\\": {\\\"Ingredient\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"quantity\\\": {\\\"title\\\": \\\"Quantity\\\", \\\"type\\\": \\\"number\\\"}, \\\"unit\\\": {\\\"title\\\": \\\"Unit\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"quantity\\\", \\\"unit\\\"], \\\"title\\\": \\\"Ingredient\\\", \\\"type\\\": \\\"object\\\"}, \\\"Instruction\\\": {\\\"properties\\\": {\\\"step_number\\\": {\\\"title\\\": \\\"Step Number\\\", \\\"type\\\": \\\"integer\\\"}, \\\"description\\\": {\\\"title\\\": \\\"Description\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"step_number\\\", \\\"description\\\"], \\\"title\\\": \\\"Instruction\\\", \\\"type\\\": \\\"object\\\"}, \\\"NutritionFacts\\\": {\\\"properties\\\": {\\\"calories\\\": {\\\"title\\\": \\\"Calories\\\", \\\"type\\\": \\\"number\\\"}, \\\"protein\\\": {\\\"title\\\": \\\"Protein\\\", \\\"type\\\": \\\"number\\\"}, \\\"fat\\\": {\\\"title\\\": \\\"Fat\\\", \\\"type\\\": \\\"number\\\"}, \\\"carbs\\\": {\\\"title\\\": \\\"Carbs\\\", \\\"type\\\": \\\"number\\\"}}, \\\"required\\\": [\\\"calories\\\", \\\"protein\\\", \\\"fat\\\", \\\"carbs\\\"], \\\"title\\\": \\\"NutritionFacts\\\", \\\"type\\\": \\\"object\\\"}}, \\\"properties\\\": {\\\"title\\\": {\\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"cuisine\\\": {\\\"title\\\": \\\"Cuisine\\\", \\\"type\\\": \\\"string\\\"}, \\\"ingredients\\\": {\\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Ingredient\\\"}, \\\"title\\\": \\\"Ingredients\\\", \\\"type\\\": \\\"array\\\"}, \\\"instructions\\\": {\\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/Instruction\\\"}, \\\"title\\\": \\\"Instructions\\\", \\\"type\\\": \\\"array\\\"}, \\\"total_time_minutes\\\": {\\\"title\\\": \\\"Total Time Minutes\\\", \\\"type\\\": \\\"integer\\\"}, \\\"nutrition\\\": {\\\"anyOf\\\": [{\\\"$ref\\\": \\\"#/$defs/NutritionFacts\\\"}, {\\\"type\\\": \\\"null\\\"}], \\\"default\\\": null}}, \\\"required\\\": [\\\"title\\\", \\\"cuisine\\\", \\\"ingredients\\\", \\\"instructions\\\", \\\"total_time_minutes\\\"]}\\n```\"\n",
    "res = llm(input)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
