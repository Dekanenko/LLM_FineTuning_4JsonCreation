{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install unsloth\n",
        "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "%pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: unknown file attribute: B\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "!wandb login os.getenv('WANDB_API')\n",
        "login(os.getenv('HUGGINGFACE_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.49.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create trainable LoRA params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.14 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v_jjMtcVhqU1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset(\"json\", data_files=\"data/train_dataset.json\", split=\"train\")\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"data/eval_dataset.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hMRAKoNKp3Yt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['input', 'output']\n",
            "['input', 'output']\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset.column_names)\n",
        "print(eval_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jqDaCOCJafH0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou are a name and surname extractor. Your task is to identify and extract the names and surnames of characters from the given text.\\n### Input:\\nIn the small town of Maplewood, Emma Kensington stumbled upon an old, dusty book while exploring her grandmother\\'s attic. Intrigued, she dusted it off to reveal the title: \"The Secrets of Eldridge Manor.\"\\n\\nHer best friend, Lucas Thompson, peeked over her shoulder, his curiosity piqued. \"Is that a ghost story? You have to read it!\"\\n\\n‚ÄúAs long as you‚Äôre ready for a late-night horror marathon!‚Äù Emma replied, teasing him. They both chuckled, remembering the last time they had tried to binge-watch scary movies.\\n\\nThat evening, the two gathered under a blanket fortress in Emma‚Äôs living room, the dusty book sprawled in front of them. As Emma began to read aloud, the surname ‚ÄúEldridge‚Äù brought shivers of nostalgia. Her grandmother often spoke of the eldritch legacy surrounding Eldridge Manor and its keeper, Victor Eldridge.\\n\\nDriven by intrigue, Emma and Lucas decided they had to visit the dilapidated manor before sunset lingered longer on the horizon. Just outside, whispers of the past danced through the evening air, beckoning them forward into a reason they had yet to unravel‚Äîthe sunny shadow of Maplewood was darkening, more hushed than before.\\n\\n### Instructions:\\n1. Extract characters\\' names and surnames clearly.\\n2. Ensure that the JSON file contains a list of unique name-surname pairs (no duplicates).\\n\\n### Format Instructions:\\n1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\\n2. Strictly adhere to the following JSON format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"$defs\": {\"FullName\": {\"properties\": {\"name\": {\"description\": \"Name of the character\", \"title\": \"Name\", \"type\": \"string\"}, \"surname\": {\"description\": \"Surname of the character\", \"title\": \"Surname\", \"type\": \"string\"}}, \"required\": [\"name\", \"surname\"], \"title\": \"FullName\", \"type\": \"object\"}}, \"properties\": {\"fullname_list\": {\"description\": \"List of full names for every character\", \"items\": {\"$ref\": \"#/$defs/FullName\"}, \"title\": \"Fullname List\", \"type\": \"array\"}}, \"required\": [\"fullname_list\"]}\\n```<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n{\\n  \"fullname_list\": [\\n    {\\n      \"name\": \"Emma\",\\n      \"surname\": \"Kensington\"\\n    },\\n    {\\n      \"name\": \"Lucas\",\\n      \"surname\": \"Thompson\"\\n    },\\n    {\\n      \"name\": \"Victor\",\\n      \"surname\": \"Eldridge\"\\n    }\\n  ]\\n}<|eot_id|>'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from src.utils import generate_prompt, generate_and_tokenize_prompt\n",
        "generate_prompt(train_dataset[0], train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "brybGUp5I-Gn"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d044ff698cb4e358adaa6eaa5bc5943",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/276 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af504dc50a17433daf7d0f1f302a50b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/41 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_dataset = train_dataset.map(lambda f: generate_and_tokenize_prompt(f, tokenizer=tokenizer))\n",
        "train_dataset = eval_dataset.map(lambda f: generate_and_tokenize_prompt(f, tokenizer=tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "k2zVuczoJFSN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input', 'output', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 41\n",
              "})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting the training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8whAtEaybNzY"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    warmup_steps = 4,\n",
        "    num_train_epochs=15,\n",
        "    learning_rate = 1e-4,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 42,\n",
        "    output_dir = \"outputs\",\n",
        "    report_to = \"wandb\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1O4tN2uP2FpC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input', 'output', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 276\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "1.088 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 276 | Num Epochs = 20 | Total steps = 340\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 11,272,192/785,713,152 (1.43% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkyrylldekanenko\u001b[0m (\u001b[33mkyrylldekanenko-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/teamspace/studios/this_studio/LLM_FineTuning_4JsonCreation/wandb/run-20250310_171124-eijc5n4y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kyrylldekanenko-/huggingface/runs/eijc5n4y' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/kyrylldekanenko-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/kyrylldekanenko-/huggingface' target=\"_blank\">https://wandb.ai/kyrylldekanenko-/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/kyrylldekanenko-/huggingface/runs/eijc5n4y' target=\"_blank\">https://wandb.ai/kyrylldekanenko-/huggingface/runs/eijc5n4y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='335' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [335/340 1:19:28 < 01:11, 0.07 it/s, Epoch 18.58/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.181600</td>\n",
              "      <td>0.822281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.787700</td>\n",
              "      <td>0.590535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.636800</td>\n",
              "      <td>0.479729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.545800</td>\n",
              "      <td>0.389069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.475600</td>\n",
              "      <td>0.352491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.448500</td>\n",
              "      <td>0.338136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.429000</td>\n",
              "      <td>0.329259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.405700</td>\n",
              "      <td>0.323506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.393000</td>\n",
              "      <td>0.315223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.380900</td>\n",
              "      <td>0.310528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>0.305807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.353800</td>\n",
              "      <td>0.303078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.335100</td>\n",
              "      <td>0.306861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.332800</td>\n",
              "      <td>0.308214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.318800</td>\n",
              "      <td>0.310772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.309000</td>\n",
              "      <td>0.314347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.315300</td>\n",
              "      <td>0.315917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.306600</td>\n",
              "      <td>0.319762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m<string>:306\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
            "File \u001b[0;32m<string>:73\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/accelerate/accelerator.py:2325\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "### Inference test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "14d1013c-6e23-4615-ab18-df2564281027"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "You are a name and surname extractor. Your task is to identify and extract the names and surnames of characters from the given text.\n",
            "### Input:\n",
            "In the quaint town of Eldridge, Sarah Quinn and Lucas Bennett stood at the park, deep in conversation. The autumn leaves crunched underfoot as they walked along the winding path. \n",
            "\n",
            "\"I can't believe we‚Äôre finally doing this,\" Lucas said, a nervous excitement in his voice.\n",
            "\n",
            "Sarah smiled, brushing a loose strand of hair behind her ear. \"I know! After months of planning, it feels surreal.\"\n",
            "\n",
            "They stopped by the old oak tree, its branches stretching wide like welcoming arms. ‚ÄúHave you thought about what you‚Äôll say?‚Äù Sarah asked, her blue eyes sparkling with mischief.\n",
            "\n",
            "‚ÄúJust that I appreciate everything he‚Äôs done for us,‚Äù Lucas replied, kicking a pebble out of their way.\n",
            "\n",
            "As they watched the sun dip below the horizon, Jack Caldwell approached, his camera slung across his shoulder. ‚ÄúThe moment‚Äôs finally here, huh?‚Äù he grinned, ready to capture the proposal.\n",
            "\n",
            "Sarah squeezed Lucas‚Äôs hand, a flutter of anticipation in her stomach as they prepared for a new chapter in their lives. Tonight, under the stars, they would begin a story of love that had been years in the making.\n",
            "\n",
            "### Instructions:\n",
            "1. Extract characters' names and surnames clearly.\n",
            "2. Ensure that the JSON file contains a list of unique name-surname pairs (no duplicates).\n",
            "\n",
            "### Format Instructions:\n",
            "1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\n",
            "2. Strictly adhere to the following JSON format:\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"FullName\": {\"properties\": {\"name\": {\"description\": \"Name of the character\", \"title\": \"Name\", \"type\": \"string\"}, \"surname\": {\"description\": \"Surname of the character\", \"title\": \"Surname\", \"type\": \"string\"}}, \"required\": [\"name\", \"surname\"], \"title\": \"FullName\", \"type\": \"object\"}}, \"properties\": {\"fullname_list\": {\"description\": \"List of full names for every character\", \"items\": {\"$ref\": \"#/$defs/FullName\"}, \"title\": \"Fullname List\", \"type\": \"array\"}}, \"required\": [\"fullname_list\"]}\n",
            "```<|eot_id|>\n",
            "\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "{\n",
            "  \"fullname_list\": [\n",
            "    {\n",
            "      \"name\": \"Sarah\",\n",
            "      \"surname\": \"Quinn\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Lucas\",\n",
            "      \"surname\": \"Bennett\"\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Jack\",\n",
            "      \"surname\": \"Caldwell\"\n",
            "    }\n",
            "  ]\n",
            "}<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "input = {\"input\":\"You are a name and surname extractor. Your task is to identify and extract the names and surnames of characters from the given text.\\n### Input:\\nIn the quaint town of Eldridge, Sarah Quinn and Lucas Bennett stood at the park, deep in conversation. The autumn leaves crunched underfoot as they walked along the winding path. \\n\\n\\\"I can't believe we‚Äôre finally doing this,\\\" Lucas said, a nervous excitement in his voice.\\n\\nSarah smiled, brushing a loose strand of hair behind her ear. \\\"I know! After months of planning, it feels surreal.\\\"\\n\\nThey stopped by the old oak tree, its branches stretching wide like welcoming arms. ‚ÄúHave you thought about what you‚Äôll say?‚Äù Sarah asked, her blue eyes sparkling with mischief.\\n\\n‚ÄúJust that I appreciate everything he‚Äôs done for us,‚Äù Lucas replied, kicking a pebble out of their way.\\n\\nAs they watched the sun dip below the horizon, Jack Caldwell approached, his camera slung across his shoulder. ‚ÄúThe moment‚Äôs finally here, huh?‚Äù he grinned, ready to capture the proposal.\\n\\nSarah squeezed Lucas‚Äôs hand, a flutter of anticipation in her stomach as they prepared for a new chapter in their lives. Tonight, under the stars, they would begin a story of love that had been years in the making.\\n\\n### Instructions:\\n1. Extract characters' names and surnames clearly.\\n2. Ensure that the JSON file contains a list of unique name-surname pairs (no duplicates).\\n\\n### Format Instructions:\\n1. Do NOT include headers, comments, or any additional text‚Äîonly generate the JSON file.\\n2. Strictly adhere to the following JSON format:\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"$defs\\\": {\\\"FullName\\\": {\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"Name of the character\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"surname\\\": {\\\"description\\\": \\\"Surname of the character\\\", \\\"title\\\": \\\"Surname\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"surname\\\"], \\\"title\\\": \\\"FullName\\\", \\\"type\\\": \\\"object\\\"}}, \\\"properties\\\": {\\\"fullname_list\\\": {\\\"description\\\": \\\"List of full names for every character\\\", \\\"items\\\": {\\\"$ref\\\": \\\"#/$defs/FullName\\\"}, \\\"title\\\": \\\"Fullname List\\\", \\\"type\\\": \\\"array\\\"}}, \\\"required\\\": [\\\"fullname_list\\\"]}\\n```\"}\n",
        "input = generate_prompt(input, train=False)\n",
        "print(input)\n",
        "\n",
        "input_ids = tokenizer(input, padding=True, return_tensors = \"pt\")[\"input_ids\"].to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 256, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "### Saving finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model_json/tokenizer_config.json',\n",
              " 'lora_model_json/special_tokens_map.json',\n",
              " 'lora_model_json/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"Llama1B_FineTuned\")\n",
        "tokenizer.save_pretrained(\"Llama1B_FineTuned\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
